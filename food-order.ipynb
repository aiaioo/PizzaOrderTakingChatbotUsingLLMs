{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a94acf7-d7e4-477d-89ea-84ffa77a3810",
   "metadata": {},
   "source": [
    "## Demo of a Pizza Ordering Chatbot using LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce8f530-6366-48ff-942e-ecbff871f463",
   "metadata": {},
   "source": [
    "This demo was created by Arvind Singh Gulati (https://www.linkedin.com/in/arvindsg/) for the NASSCOM Engineering and Design Summit keynote lecture on a pizza order-taking chatbot implementation using ChatGPT.  The video of Arvind demoing this order-taking chatbot is on Youtube at https://www.youtube.com/watch?v=2gkWBz52Yzg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "959b8348-fb4b-4ee0-b96b-063baf998ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydantic in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (2.8.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Collecting pydantic\n",
      "  Downloading pydantic-2.9.0-py3-none-any.whl.metadata (146 kB)\n",
      "Requirement already satisfied: openai in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (1.43.0)\n",
      "Collecting openai\n",
      "  Downloading openai-1.44.0-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: langchain in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (0.2.15)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.2.16-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (0.2.15)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.2.16-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: langgraph in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (0.2.16)\n",
      "Collecting langgraph\n",
      "  Downloading langgraph-0.2.19-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: langchain-anthropic in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (0.1.23)\n",
      "Requirement already satisfied: langchain-openai in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (0.1.23)\n",
      "Requirement already satisfied: langgraph-checkpoint-sqlite in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (1.0.2)\n",
      "Collecting langgraph-checkpoint-sqlite\n",
      "  Downloading langgraph_checkpoint_sqlite-1.0.3-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from pydantic) (0.7.0)\n",
      "Collecting pydantic-core==2.23.2 (from pydantic)\n",
      "  Downloading pydantic_core-2.23.2-cp312-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from pydantic) (4.12.2)\n",
      "Collecting tzdata (from pydantic)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from openai) (0.5.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from langchain) (2.0.32)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from langchain) (3.10.5)\n",
      "Collecting langchain-core<0.3.0,>=0.2.38 (from langchain)\n",
      "  Downloading langchain_core-0.2.38-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from langchain) (0.2.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from langchain) (0.1.108)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: langgraph-checkpoint<2.0.0,>=1.0.2 in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from langgraph) (1.0.8)\n",
      "Requirement already satisfied: anthropic<1,>=0.30.0 in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from langchain-anthropic) (0.34.1)\n",
      "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from langchain-anthropic) (0.7.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: aiosqlite<0.21.0,>=0.20.0 in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from langgraph-checkpoint-sqlite) (0.20.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.7)\n",
      "Requirement already satisfied: tokenizers>=0.13.0 in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from anthropic<1,>=0.30.0->langchain-anthropic) (0.20.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.8)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.38->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.38->langchain) (24.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.7.24)\n",
      "Requirement already satisfied: colorama in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.38->langchain) (3.0.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from tokenizers>=0.13.0->anthropic<1,>=0.30.0->langchain-anthropic) (0.24.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<1,>=0.30.0->langchain-anthropic) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\orion\\anaconda3\\envs\\py312\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<1,>=0.30.0->langchain-anthropic) (2024.6.1)\n",
      "Downloading pydantic-2.9.0-py3-none-any.whl (434 kB)\n",
      "Downloading pydantic_core-2.23.2-cp312-none-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   -------------------------------- ------- 1.6/1.9 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 7.5 MB/s eta 0:00:00\n",
      "Downloading openai-1.44.0-py3-none-any.whl (367 kB)\n",
      "Downloading langchain-0.2.16-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/1.0 MB 5.9 MB/s eta 0:00:00\n",
      "Downloading langchain_community-0.2.16-py3-none-any.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   -------------------------- ------------- 1.6/2.3 MB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.3/2.3 MB 7.0 MB/s eta 0:00:00\n",
      "Downloading langgraph-0.2.19-py3-none-any.whl (97 kB)\n",
      "Downloading langgraph_checkpoint_sqlite-1.0.3-py3-none-any.whl (12 kB)\n",
      "Downloading langchain_core-0.2.38-py3-none-any.whl (396 kB)\n",
      "Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Installing collected packages: tzdata, pydantic-core, pydantic, openai, langchain-core, langgraph-checkpoint-sqlite, langgraph, langchain, langchain-community\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.20.1\n",
      "    Uninstalling pydantic_core-2.20.1:\n",
      "      Successfully uninstalled pydantic_core-2.20.1\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.8.2\n",
      "    Uninstalling pydantic-2.8.2:\n",
      "      Successfully uninstalled pydantic-2.8.2\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.43.0\n",
      "    Uninstalling openai-1.43.0:\n",
      "      Successfully uninstalled openai-1.43.0\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.2.37\n",
      "    Uninstalling langchain-core-0.2.37:\n",
      "      Successfully uninstalled langchain-core-0.2.37\n",
      "  Attempting uninstall: langgraph-checkpoint-sqlite\n",
      "    Found existing installation: langgraph-checkpoint-sqlite 1.0.2\n",
      "    Uninstalling langgraph-checkpoint-sqlite-1.0.2:\n",
      "      Successfully uninstalled langgraph-checkpoint-sqlite-1.0.2\n",
      "  Attempting uninstall: langgraph\n",
      "    Found existing installation: langgraph 0.2.16\n",
      "    Uninstalling langgraph-0.2.16:\n",
      "      Successfully uninstalled langgraph-0.2.16\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.2.15\n",
      "    Uninstalling langchain-0.2.15:\n",
      "      Successfully uninstalled langchain-0.2.15\n",
      "  Attempting uninstall: langchain-community\n",
      "    Found existing installation: langchain-community 0.2.15\n",
      "    Uninstalling langchain-community-0.2.15:\n",
      "      Successfully uninstalled langchain-community-0.2.15\n",
      "Successfully installed langchain-0.2.16 langchain-community-0.2.16 langchain-core-0.2.38 langgraph-0.2.19 langgraph-checkpoint-sqlite-1.0.3 openai-1.44.0 pydantic-2.9.0 pydantic-core-2.23.2 tzdata-2024.1\n"
     ]
    }
   ],
   "source": [
    "%pip install -U pydantic openai langchain langchain-community langgraph langchain-anthropic langchain-openai langgraph-checkpoint-sqlite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3355ff96-584a-40d0-ad5b-eb1ce9d999d7",
   "metadata": {},
   "source": [
    "You need to get yourself either an OpenAI key (for GPT-4o-mini) or an Anthropic key (for the Claude Sonnet LLM which has a free tier - Haiku does not work very well for this task).\n",
    "The OpenAI API responds way faster and works very well, but you have to pay for it from the beginning (very cheap though - around 1.5 dollars for a million characters as of September 2024).  Once you change the following box (replacing the text '<your key>' with the API key in quotes) you can run the rest of the code to the end - no changes needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c46b4d7e-43e0-447d-8215-ce5f9158a0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = '<your key>'              # Use this preferrably - it's faster\n",
    "assert \"OPENAI_API_KEY\" in os.environ\n",
    "#os.environ['ANTHROPIC_API_KEY'] = '<your key>'\n",
    "#assert \"ANTHROPIC_API_KEY\" in os.environ\n",
    "#assert \"LANGCHAIN_API_KEY\" in os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd5ec1b3-12e8-429f-a3e9-88bd5230a1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI                  # Use this preferrably - faster\n",
    "from langchain_anthropic import ChatAnthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73a8c9c9-d488-47f9-a499-115bf8bc2fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"  # Make true for LangSmith\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"nasscom\"\n",
    "# LANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "786fe4ca-55eb-4d14-b594-46e290a3a0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_SIZES = [\"small\", \"medium\", \"large\"]\n",
    "VALID_TOPPINGS = [\"capsicum\", \"tomatoes\", \"olives\", \"mushrooms\", \"onions\", \"jalapenos\", \"pineapple\", \"pepperoni\"]\n",
    "VALID_SIDES = [\"garlic bread\", \"choco lava cake\", \"chicken taco\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdc8cfb4-1e70-42bd-9de2-1c1692d14806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.tools import tool\n",
    "\n",
    "order_count = 0\n",
    "order_status_db={}\n",
    "\n",
    "\n",
    "\n",
    "def pizza_order(pizza_size: str, pizza_toppings: List[str], side_items: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Places an order for a pizza with the specified size, toppings, and side items. Must be invoked once the order details are confirmed by the user\n",
    "\n",
    "    Args:\n",
    "        pizza_size (str): The size of the pizza to order (e.g., 'small', 'medium', 'large').\n",
    "        pizza_toppings (List[str]): A list of toppings to add to the pizza.\n",
    "        side_items (List[str]): A list of side items to include with the order.\n",
    "\n",
    "    Returns:\n",
    "        str: A message indicating whether the order was placed successfully, or an error message if there were invalid inputs.\n",
    "\n",
    "    Example:\n",
    "        >>> pizza_order('large', ['pepperoni', 'mushrooms'], ['garlic bread', 'soda'])\n",
    "        \"Your order has been placed successfully! Here are the details:\\n{'pizza_size': 'large', 'pizza_toppings': ['pepperoni', 'mushrooms'], 'side_items': ['garlic bread', 'soda']}\"\n",
    "    \"\"\"\n",
    "\n",
    "    global order_count\n",
    "    \n",
    "    # Basic validation and order processing logic\n",
    "    if pizza_size not in VALID_SIZES:\n",
    "        return f\"Error: Invalid pizza size '{pizza_size}'. Please choose from {VALID_SIZES}.\"\n",
    "    \n",
    "    invalid_toppings = [topping for topping in pizza_toppings if topping not in VALID_TOPPINGS]\n",
    "    if invalid_toppings:\n",
    "        return f\"Error: Invalid toppings {invalid_toppings}. Please choose from {VALID_TOPPINGS}.\"\n",
    "    \n",
    "    invalid_sides = [side for side in side_items if side not in VALID_SIDES]\n",
    "    if invalid_sides:\n",
    "        return f\"Error: Invalid side items {invalid_sides}. Please choose from {VALID_SIDES}.\"\n",
    "    order_count+=1\n",
    "    \n",
    "    order_details = {\n",
    "        \"pizza_size\": pizza_size,\n",
    "        \"pizza_toppings\": pizza_toppings,\n",
    "        \"side_items\": side_items,\n",
    "        \"order_id\": order_count,\n",
    "    }\n",
    "    order_status_db[order_count] = \"pending\"\n",
    "    return f\"Your order has been placed successfully! Your order id is: {order_count}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd442a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def order_status(order_id: int) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves the status of a pizza order based on the order ID.\n",
    "\n",
    "    Args:\n",
    "        order_id (int): The ID of the order to check the status of.\n",
    "\n",
    "    Returns:\n",
    "        str: A message indicating the status of the order, or an error message if the order ID is invalid.\n",
    "\n",
    "    Example:\n",
    "        >>> order_status(1)\n",
    "        \"Order 1 is pending.\"\n",
    "    \"\"\"\n",
    "\n",
    "    if order_id not in order_status_db:\n",
    "        return f\"Error: Order ID {order_id} not found.\"\n",
    "    \n",
    "    return f\"Order {order_id} is {order_status_db[order_id]}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f882f45-2538-4e27-8e8a-fb0e3067cd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "pizza_doc=f\"\"\"- The pizzas are freshly baked using wheat base and will be served within 10 minutes of ordering.\n",
    "- Pizza is available in one of the three sizes:-  {\", \".join(VALID_SIZES)}\n",
    "- Any number of toppings can be added to a pizza\n",
    "- Pizza toppings can be one of these:- {\", \".join(VALID_TOPPINGS)}\n",
    "- The large pizza should be enough for about 4 people\n",
    "- The medium pizza should be enough for about 2 people\n",
    "- The small pizza should be enough for about 1 person\n",
    "- Allergen Information:\n",
    "    - The choco lava cake contains peanut oil\n",
    "    - The chick taco contains soya\n",
    "    - all products include milk based ingredients.\n",
    "- We do not have an option for extra cheese on the pizza\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb95688d-43ab-4598-9fb2-53869771b724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- The pizzas are freshly baked using wheat base and will be served within 10 minutes of ordering.\n",
      "- Pizza is available in one of the three sizes:-  small, medium, large\n",
      "- Any number of toppings can be added to a pizza\n",
      "- Pizza toppings can be one of these:- capsicum, tomatoes, olives, mushrooms, onions, jalapenos, pineapple, pepperoni\n",
      "- The large pizza should be enough for about 4 people\n",
      "- The medium pizza should be enough for about 2 people\n",
      "- The small pizza should be enough for about 1 person\n",
      "- Allergen Information:\n",
      "    - The choco lava cake contains peanut oil\n",
      "    - The chick taco contains soya\n",
      "    - all products include milk based ingredients.\n",
      "- We do not have an option for extra cheese on the pizza\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pizza_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5a0a96d-cb5a-4cdb-a915-6eb7b62b679a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- The pizzas are freshly baked using wheat base and will be served within 10 minutes of ordering.\n",
       "- Pizza is available in one of the three sizes:-  small, medium, large\n",
       "- Any number of toppings can be added to a pizza\n",
       "- Pizza toppings can be one of these:- capsicum, tomatoes, olives, mushrooms, onions, jalapenos, pineapple, pepperoni\n",
       "- The large pizza should be enough for about 4 people\n",
       "- The medium pizza should be enough for about 2 people\n",
       "- The small pizza should be enough for about 1 person\n",
       "- Allergen Information:\n",
       "    - The choco lava cake contains peanut oil\n",
       "    - The chick taco contains soya\n",
       "    - all products include milk based ingredients.\n",
       "- We do not have an option for extra cheese on the pizza\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "display(Markdown(pizza_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e26db36-9c58-4b60-aaf4-949046782974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a friendly but rule bound pizza ordering assistant. You should respond to the user query based on the following information:-\\n- The pizzas are freshly baked using wheat base and will be served within 10 minutes of ordering.\\n- Pizza is available in one of the three sizes:-  small, medium, large\\n- Any number of toppings can be added to a pizza\\n- Pizza toppings can be one of these:- capsicum, tomatoes, olives, mushrooms, onions, jalapenos, pineapple, pepperoni\\n- The large pizza should be enough for about 4 people\\n- The medium pizza should be enough for about 2 people\\n- The small pizza should be enough for about 1 person\\n- Allergen Information:\\n    - The choco lava cake contains peanut oil\\n    - The chick taco contains soya\\n    - all products include milk based ingredients.\\n- We do not have an option for extra cheese on the pizza\\n\\nIn case the user query cannot be responded to based on the above information, Respond by literally saying \"I am not sure about this\"\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_response_prompt=f\"\"\"You are a friendly but rule bound pizza ordering assistant. You should respond to the user query based on the following information:-\n",
    "{pizza_doc}\n",
    "In case the user query cannot be responded to based on the above information, Respond by literally saying \"I am not sure about this\"\n",
    "\"\"\"\n",
    "question_response_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23f756f0-ff3b-41a1-9be4-7765f6048876",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\",temperature=0)                            # To use this you need to have the OpenAI key\n",
    "#llm= ChatAnthropic(model=\"claude-3-5-sonnet-20240620\",temperature=0)\n",
    "# llm=ChatAnthropic(model=\"claude-3-haiku-20240307\",temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94a97c66-3b51-47b1-8181-95486334b1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import AnyMessage,add_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ea9ea6c-d4ee-4c2e-a54d-09e5a53f5bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "    size:str\n",
    "    toppings:list[str]\n",
    "    sides:list[str]\n",
    "    confirmed:bool\n",
    "    next:str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c17c04e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a supervisor tasked with picking the right worker to act upon user queries. You may allocate the user query to one the following workers:-\n",
      "order_taker: This worker keep track of user's pizza order, specifically tracks the size of pizza,list of toppings and side items. Choose this worker if the user query is related to updating the order\n",
      "helper: Answers general user queries about pizza and order. Choose this worker only if of none of the other workers are suitable to respond\n",
      "order_status: This worker keeps track of the status of the order. Choose this worker if the user query is related to the status of the order\n",
      "Always respond concisely ONLY with the name of the worker most suitable to respond to the most recent user query\n"
     ]
    }
   ],
   "source": [
    "helper_nodes={\n",
    "    \"order_taker\": \"This worker keep track of user's pizza order, specifically tracks the size of pizza,list of toppings and side items. Choose this worker if the user query is related to updating the order\",\n",
    "    \"helper\":\"Answers general user queries about pizza and order. Choose this worker only if of none of the other workers are suitable to respond\",\n",
    "    \"order_status\":\"This worker keeps track of the status of the order. Choose this worker if the user query is related to the status of the order\",\n",
    "}\n",
    "worker_descriptions = \"\\n\".join([f\"{name}: {description}\" for name, description in helper_nodes.items()])\n",
    "router_agent_prompt=\"You are a supervisor tasked with picking the right worker to act upon user queries. You may allocate the user query to one the following workers:-\\n\" \\\n",
    "+ worker_descriptions \\\n",
    "+\"\\nAlways respond concisely ONLY with the name of the worker most suitable to respond to the most recent user query\"\n",
    "\n",
    "\n",
    "print(router_agent_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a49b576e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Literal,Optional\n",
    "# options = [\"FINISH\"] + list(helper_nodes.keys())\n",
    "options = list(helper_nodes.keys())\n",
    "\n",
    "class routeResponse(BaseModel):\n",
    "    next: Literal[*options]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b390df21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class orderTakeResponse(BaseModel):\n",
    "    size:Optional[Literal[*VALID_SIZES]]\n",
    "    toppings:List[Literal[*VALID_TOPPINGS]]\n",
    "    sides:List[Literal[*VALID_SIDES]]\n",
    "    confirmed:bool\n",
    "    response:str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c04c9c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e3cbbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "router_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", router_agent_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        \n",
    "    ]\n",
    ").partial(options=str(options))\n",
    "\n",
    "def supervisor_agent(state):\n",
    "    supervisor_chain = (\n",
    "        router_prompt\n",
    "        | llm.with_structured_output(routeResponse)\n",
    "    )\n",
    "    return supervisor_chain.invoke(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d340afb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_node(state, agent, name):\n",
    "    result = agent(state)\n",
    "    \n",
    "    if hasattr(result,\"tool_calls\") and result.tool_calls:\n",
    "        print(\"Requested tool call\", result.tool_calls)\n",
    "    else:\n",
    "        print(\"No tool call requested\")\n",
    "    if hasattr(result,\"content\"):\n",
    "        return {\"messages\": [AIMessage(content=result.content, name=name)]}\n",
    "    elif hasattr(result,\"response\"):\n",
    "        return {\"messages\": [AIMessage(content=result.response, name=name)]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "139625a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    return {\"messages\": [HumanMessage(content=result[\"messages\"][-1].content, name=name)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "752a12af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_taker_node_factory(state,agent,name):\n",
    "    result = agent(state)\n",
    "    response={}\n",
    "    response[\"messages\"]=[HumanMessage(content=result.response, name=name)]\n",
    "    response[\"size\"]=result.size\n",
    "    response[\"toppings\"]=result.toppings\n",
    "    response[\"sides\"]=result.sides\n",
    "    response[\"confirmed\"]=result.confirmed\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "585457fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", question_response_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "def helper_agent(state):\n",
    "    helper_chain = (\n",
    "        helper_prompt\n",
    "        | llm\n",
    "    )\n",
    "    return helper_chain.invoke(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "252d3dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_taker_prompt=\"You are a professional, concise server responsible for taking the user's pizza order. Only following information is relevant about the order:-\\n\" + \\\n",
    "f\"- Size of the pizza, it should be one of the these:- {', '.join(VALID_SIZES)} \\n\" + \\\n",
    "f\"- Toppings to be added to the pizza, User may only add on or some of these:- {', '.join(VALID_TOPPINGS)}\\n\" + \\\n",
    "f\"- Side items to be added to the order, User may only add on or some of these:- {', '.join(VALID_SIDES)}\\n\" + \\\n",
    "\"Acknowledge user's request by making an acceptance statement but in case the user request is confusing, feel free to ask clarifying question. In case user has not provided all information regarding a order, ask for it one at a time. Ensure that you ask the user If he wants to add any additional toppings or side items unless that have already confirmed that they don't. Maintain the continuity of the conversation and do *not* jumping across topics. Be patient and only ask one question in a single response.\" +\\\n",
    "\"In case the user has asked any clarifying questions, respond to them on the basis of following information only:-\\n\" + \\\n",
    "f\"{pizza_doc}\" +\\\n",
    "\"In case the user query cannot be responded to based on the above information, Respond by literally saying 'I am not sure about this'\" +\\\n",
    "\"*Respond in the form of a json object with following schema*:-\\n\" + \\\n",
    "\"\"\"{{\n",
    "    \"pizza_size\": str, // The size of the pizza to order\n",
    "    \"pizza_toppings\": List[str], // A list of toppings to add to the pizza\n",
    "    \"side_items\": List[str] // A list of side items to include with the order\n",
    "    \"response\": str //A response to the user based on the last update, It may be clarifying question about the user's request, a confirmation message or a probing question about the orde. Be concise and to the point\n",
    "    \"confirmed:bool // A boolean value indicating whether all information is available and confirmed from the user. One you set this to true, the order will be placed and cannot be updated further\n",
    "\n",
    "}}\"\"\" +\\\n",
    "\"Remember to *always* respond to the user query in the form of a json object with the above schema only. \"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f2c5b1c-52d0-4d4a-946a-626e8c7ff1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a professional, concise server responsible for taking the user's pizza order. Only following information is relevant about the order:-\n",
      "- Size of the pizza, it should be one of the these:- small, medium, large \n",
      "- Toppings to be added to the pizza, User may only add on or some of these:- capsicum, tomatoes, olives, mushrooms, onions, jalapenos, pineapple, pepperoni\n",
      "- Side items to be added to the order, User may only add on or some of these:- garlic bread, choco lava cake, chicken taco\n",
      "Acknowledge user's request by making an acceptance statement but in case the user request is confusing, feel free to ask clarifying question. In case user has not provided all information regarding a order, ask for it one at a time. Ensure that you ask the user If he wants to add any additional toppings or side items unless that have already confirmed that they don't. Maintain the continuity of the conversation and do *not* jumping across topics. Be patient and only ask one question in a single response.In case the user has asked any clarifying questions, respond to them on the basis of following information only:-\n",
      "- The pizzas are freshly baked using wheat base and will be served within 10 minutes of ordering.\n",
      "- Pizza is available in one of the three sizes:-  small, medium, large\n",
      "- Any number of toppings can be added to a pizza\n",
      "- Pizza toppings can be one of these:- capsicum, tomatoes, olives, mushrooms, onions, jalapenos, pineapple, pepperoni\n",
      "- The large pizza should be enough for about 4 people\n",
      "- The medium pizza should be enough for about 2 people\n",
      "- The small pizza should be enough for about 1 person\n",
      "- Allergen Information:\n",
      "    - The choco lava cake contains peanut oil\n",
      "    - The chick taco contains soya\n",
      "    - all products include milk based ingredients.\n",
      "- We do not have an option for extra cheese on the pizza\n",
      "In case the user query cannot be responded to based on the above information, Respond by literally saying 'I am not sure about this'*Respond in the form of a json object with following schema*:-\n",
      "{{\n",
      "    \"pizza_size\": str, // The size of the pizza to order\n",
      "    \"pizza_toppings\": List[str], // A list of toppings to add to the pizza\n",
      "    \"side_items\": List[str] // A list of side items to include with the order\n",
      "    \"response\": str //A response to the user based on the last update, It may be clarifying question about the user's request, a confirmation message or a probing question about the orde. Be concise and to the point\n",
      "    \"confirmed:bool // A boolean value indicating whether all information is available and confirmed from the user. One you set this to true, the order will be placed and cannot be updated further\n",
      "\n",
      "}}Remember to *always* respond to the user query in the form of a json object with the above schema only. \n"
     ]
    }
   ],
   "source": [
    "print(order_taker_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97c52f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", order_taker_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "def order_taker_agent(state):\n",
    "    order_take_chain = (\n",
    "        order_prompt\n",
    "        | llm.bind_tools([pizza_order]).with_structured_output(orderTakeResponse) #add tool call here\n",
    "    )\n",
    "    return order_take_chain.invoke(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d748acbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unimplemented_agent_node(state,name):\n",
    "    return {\"messages\": [HumanMessage(content=\"This will be replaced by real message\",name=name)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96314296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def place_order_node_factory(state,name):\n",
    "    response=pizza_order(state[\"size\"],state[\"toppings\"],state[\"sides\"])\n",
    "    return {\"messages\": [AIMessage(content=response,name=name)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8998dc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the graph\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "import functools\n",
    "from langgraph.prebuilt import ToolNode,tools_condition\n",
    "\n",
    "\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"supervisor\", supervisor_agent)\n",
    "helper_node=functools.partial(agent_node, agent=helper_agent, name=\"helper\")\n",
    "workflow.add_node(\"helper\", helper_node)\n",
    "order_taker_node=functools.partial(order_taker_node_factory,agent=order_taker_agent, name=\"order_taker\")\n",
    "workflow.add_node(\"order_taker\", order_taker_node)\n",
    "\n",
    "place_order_node=functools.partial(place_order_node_factory, name=\"order_place\")\n",
    "workflow.add_node(\"order_place\", place_order_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582ae58a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ddc5963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "tools=[order_status]\n",
    "model=llm.bind_tools(tools)\n",
    "order_status_agent = create_react_agent(model, tools)\n",
    "order_status_node=functools.partial(invoke_agent_node, agent=order_status_agent, name=\"order_status\")\n",
    "workflow.add_node(\"order_status\", order_status_node)\n",
    "tool_node = ToolNode(tools=tools)\n",
    "workflow.add_node(\"tools\", tool_node)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e11d4150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_place_condition(state):\n",
    "    if state[\"confirmed\"]:\n",
    "        return \"order_place\"\n",
    "    else:\n",
    "        return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3449c30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from superviser to routed nodes    \n",
    "workflow.add_conditional_edges(\"supervisor\", lambda x: x[\"next\"],{\"order_taker\":\"order_taker\",\"helper\":\"helper\",\"order_status\":\"order_status\"})\n",
    "workflow.add_conditional_edges(\"order_taker\",order_place_condition,{\"order_place\":\"order_place\",END:END})\n",
    "workflow.add_conditional_edges(\"order_status\",tools_condition,{\"tools\":\"tools\",END:END})\n",
    "workflow.add_edge(\"order_place\", END)\n",
    "workflow.add_edge(\"helper\", END)\n",
    "workflow.add_edge(START, \"supervisor\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "18787aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc97ddf3-e911-4dc3-8449-8eb602c3d8e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1e7d0635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_status_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3ba17a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'supervisor': {'next': 'order_taker'}}\n",
      "----\n",
      "{'order_taker': {'messages': [HumanMessage(content=\"Certainly! I'd be happy to help you order a large pizza. Would you like to add any toppings to your pizza? We have options like capsicum, tomatoes, olives, mushrooms, onions, jalapenos, pineapple, and pepperoni.\", name='order_taker', id='bf47910e-0dab-4466-94fa-f42ce2941241')], 'size': 'large', 'toppings': [], 'sides': [], 'confirmed': False}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "events = graph.stream({\n",
    "        \"messages\": [            \n",
    "            HumanMessage(content=\"I want to order a large pizza\"),\n",
    "        ],\n",
    "    },\n",
    "    # Maximum number of steps to take in the graph\n",
    "    {\"recursion_limit\": 3},\n",
    ")\n",
    "for s in events:\n",
    "    print(s)\n",
    "    print(\"----\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6864788e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_status_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1a6c9ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'supervisor': {'next': 'order_taker'}}\n",
      "----\n",
      "{'order_taker': {'messages': [HumanMessage(content=\"Excellent! Your order has been confirmed. You've ordered a large pizza with onion, capsicum, and tomato toppings, along with a side of garlic bread. Your freshly baked pizza will be ready within 10 minutes. Thank you for your order, and enjoy your meal!\", name='order_taker', id='0a2a2c8b-68e9-4f69-adea-2bab6f435db4')], 'size': 'large', 'toppings': ['onions', 'capsicum', 'tomatoes'], 'sides': ['garlic bread'], 'confirmed': True}}\n",
      "----\n",
      "{'order_place': {'messages': [AIMessage(content='Your order has been placed successfully! Your order id is: 1', name='order_place')]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "        HumanMessage(content=\"I want to order a medium pizza with onion, capsicum and tomato toppings\"),\n",
    "        AIMessage(content=\"Great choice! Would you like to add any side items, such as garlic bread, choco lava cake, or chicken taco?\"),\n",
    "        HumanMessage(content=\"Yes, add a garlic bread. Also, is a medium pizza enough for 3 people?\"),\n",
    "        AIMessage(content=\"A medium pizza is generally enough for about 2 people. Would you like to consider ordering a large pizza instead?\"),\n",
    "        HumanMessage(content=\"Yes\"),\n",
    "        AIMessage(content=\"I've updated your order to a large pizza with onion, capsicum, and tomato toppings, along with a garlic bread. Would you like to add any additional toppings or side items?\"),\n",
    "        HumanMessage(content=\"No. That's all.\"),\n",
    "        AIMessage(content=\"Thank you for your order! Just to confirm, you've ordered a large pizza with onion, capsicum, and tomato toppings, along with a garlic bread. Is this correct? If so, I'll place your order.\"),\n",
    "        HumanMessage(content=\"Yes, that's correct.\")\n",
    "\n",
    "]\n",
    "events = graph.stream({\n",
    "        \"messages\": messages,\n",
    "    },\n",
    "    # Maximum number of steps to take in the graph\n",
    "    {\"recursion_limit\": 5},\n",
    ")\n",
    "for s in events:\n",
    "    print(s)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "edeec6ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'pending'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_status_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ea16dd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'supervisor': {'next': 'order_taker'}}\n",
      "----\n",
      "{'order_taker': {'messages': [HumanMessage(content=\"Certainly! I've confirmed your order for a medium pizza with onion, capsicum, and tomato toppings, and no sides. Your order has been placed successfully. Is there anything else you'd like to know about your order?\", name='order_taker', id='8aa840a9-ac0f-493c-a175-e826bf7ae7bb')], 'size': 'medium', 'toppings': ['onions', 'capsicum', 'tomatoes'], 'sides': [], 'confirmed': True}}\n",
      "----\n",
      "{'order_place': {'messages': [AIMessage(content='Your order has been placed successfully! Your order id is: 2', name='order_place')]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "        HumanMessage(content=\"I want to order a medium pizza with onion, capsicum and tomato toppings and no sides. Please consider the order confirmed\"),        \n",
    "\n",
    "]\n",
    "events = graph.stream({\n",
    "        \"messages\": messages,\n",
    "    },\n",
    "    # Maximum number of steps to take in the graph\n",
    "    {\"recursion_limit\": 5},\n",
    ")\n",
    "for s in events:\n",
    "    print(s)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "58fa7b1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'pending', 2: 'pending'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_status_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3f0c9dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'supervisor': {'next': 'order_status'}}\n",
      "----\n",
      "{'order_status': {'messages': [HumanMessage(content=\"Certainly! I'd be happy to help you check the status of your order. However, to retrieve the status, I'll need the order ID. The order ID is a unique number assigned to your pizza order when it was placed.\\n\\nCould you please provide me with your order ID? It's usually a numerical value that you would have received when you placed your order. Once you give me the order ID, I can use it to check the status of your order.\", name='order_status', id='5ecd5213-e3a8-43d1-b3bd-0bce11929067')]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "        HumanMessage(content=\"I want to know the status of my order\"),\n",
    "]\n",
    "events = graph.stream({\n",
    "        \"messages\": messages,\n",
    "    },\n",
    "    # Maximum number of steps to take in the graph\n",
    "    {\"recursion_limit\": 5},\n",
    ")\n",
    "for s in events:\n",
    "    print(s)\n",
    "    print(\"----\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "58663834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'pending', 2: 'pending'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_status_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "35b8e93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'supervisor': {'next': 'order_status'}}\n",
      "----\n",
      "{'order_status': {'messages': [HumanMessage(content=\"Based on the information I've received, I can inform you that Order 1 is currently pending. This means that your order has been received and is being processed, but it hasn't been completed or shipped yet.\\n\\nIs there anything else you'd like to know about your order or any other assistance I can provide?\", name='order_status', id='ebd33648-c291-4fc3-a705-d7cf83376460')]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "        HumanMessage(content=\"I want to know the status of my order\"),\n",
    "        AIMessage(content=\"Certainly! I'd be happy to help you check the status of your order. However, to retrieve the status, I'll need the order ID. The order ID is typically a number assigned to your order when you placed it.\\n\\nCould you please provide me with your order ID? Once I have that information, I can use our system to check the status for you.\"),\n",
    "        HumanMessage(content=\"My order id is 1\")\n",
    "]\n",
    "events = graph.stream({\n",
    "        \"messages\": messages,\n",
    "    },\n",
    "    # Maximum number of steps to take in the graph\n",
    "    {\"recursion_limit\": 5},\n",
    ")\n",
    "for s in events:\n",
    "    print(s)\n",
    "    print(\"----\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "64527417",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_status_db[1]=\"Dispatched\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7b4522a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'supervisor': {'next': 'order_status'}}\n",
      "----\n",
      "{'order_status': {'messages': [HumanMessage(content=\"Great news! I've retrieved the status of your order. The system shows that Order 1 is Dispatched. \\n\\nThis means that your order has been prepared and is now on its way to you. It has left our facility and is in the delivery process. You should receive it soon, depending on your location and the delivery method chosen.\\n\\nIs there anything else you'd like to know about your order or any other assistance I can provide?\", name='order_status', id='068cf21b-00f3-4287-939b-9db0f2ea66cd')]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "        HumanMessage(content=\"I want to know the status of my order\"),\n",
    "        AIMessage(content=\"Certainly! I'd be happy to help you check the status of your order. However, to retrieve the status, I'll need the order ID. The order ID is typically a number assigned to your order when you placed it.\\n\\nCould you please provide me with your order ID? Once I have that information, I can use our system to check the status for you.\"),\n",
    "        HumanMessage(content=\"My order id is 1\")\n",
    "]\n",
    "events = graph.stream({\n",
    "        \"messages\": messages,\n",
    "    },\n",
    "    # Maximum number of steps to take in the graph\n",
    "    {\"recursion_limit\": 5},\n",
    ")\n",
    "for s in events:\n",
    "    print(s)\n",
    "    print(\"----\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e9d61000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'supervisor': {'next': 'order_taker'}}\n",
      "----\n",
      "{'order_taker': {'messages': [HumanMessage(content=\"Certainly! I'd be happy to help you order a pizza with onion, capsicum, and tomato toppings. That sounds delicious! I've noted down your toppings. Could you please tell me what size pizza you'd like? We have small, medium, and large options available.\", name='order_taker', id='125cafa8-5bba-46e1-9237-8f325103ee04')], 'size': 'medium', 'toppings': ['onions', 'capsicum', 'tomatoes'], 'sides': [], 'confirmed': False}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "        HumanMessage(content=\"I want to order a pizza with onion, capsicum and tomato toppings.\"),        \n",
    "\n",
    "]\n",
    "events = graph.stream({\n",
    "        \"messages\": messages,\n",
    "    },\n",
    "    # Maximum number of steps to take in the graph\n",
    "    {\"recursion_limit\": 5},\n",
    ")\n",
    "for s in events:\n",
    "    print(s)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1d222af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive():\n",
    "    message=input(\"Welcome to the Robo Pizza Hut. How can I help you today?\")\n",
    "    print(\"Welcome to the Robo Pizza Hut. How can I help you today?\")\n",
    "    messages = [\n",
    "            HumanMessage(content=message),        \n",
    "        ]\n",
    "    while(True):        \n",
    "        if message in [\"exit\",\"q\",\"quit\"]:\n",
    "            break\n",
    "        messages.append(HumanMessage(content=message))\n",
    "        print(message)\n",
    "        events = graph.stream({\n",
    "                \"messages\": messages,\n",
    "            },\n",
    "            # Maximum number of steps to take in the graph\n",
    "            {\"recursion_limit\": 5},\n",
    "        )\n",
    "        events=list(events)\n",
    "        # for s in events:\n",
    "            # print(s)\n",
    "            # print(\"----\")\n",
    "        if len(events)>0:            \n",
    "            last_event=events[-1]\n",
    "            v=list(last_event.values())[-1]\n",
    "            if \"messages\" in v:\n",
    "                m=v[\"messages\"][-1]\n",
    "                messages.append(AIMessage(m.content)) \n",
    "                print(m.content)            \n",
    "            message=input()\n",
    "        else:\n",
    "            print(\"No event recieved\")\n",
    "            print(events)\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ac53b291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Robo Pizza Hut. How can I help you today?\n",
      "I want to order a large pizza with no toppings, no sides. Order is confirmed.\n",
      "{'supervisor': {'next': 'order_taker'}}\n",
      "----\n",
      "{'order_taker': {'messages': [HumanMessage(content=\"Certainly! I've confirmed your order for a large pizza with no toppings and no side items. Your order has been placed and will be prepared shortly. Is there anything else you need?\", name='order_taker', id='40de9027-a6f0-447b-a0ae-fd570f0513a8')], 'size': 'large', 'toppings': [], 'sides': [], 'confirmed': True}}\n",
      "----\n",
      "{'order_place': {'messages': [AIMessage(content='Your order has been placed successfully! Your order id is:\\n8', name='order_place', id='9f7c21f8-ce47-4ac1-b77e-bb3cc223113a')]}}\n",
      "----\n",
      "---> Your order has been placed successfully! Your order id is:\n",
      "8\n",
      "Where is my food\n",
      "{'supervisor': {'next': 'order_status'}}\n",
      "----\n",
      "{'order_status': {'messages': [HumanMessage(content='Based on the result from the order_status function, I can tell you that your order (Order 8) is currently pending. This means that your large pizza with no toppings has been received by the restaurant and is in the process of being prepared.\\n\\nThe status \"pending\" typically means that the order has been accepted but hasn\\'t yet moved to the cooking or delivery stage. The restaurant is likely working on it, and it should progress to the next stages soon.\\n\\nIf you\\'d like to check the status again later or if you have any other questions about your order, please don\\'t hesitate to ask. You can always use your order ID (8) to inquire about the latest status.', name='order_status', id='e59befc0-b01c-4ed4-8f46-5de61bcd6f31')]}}\n",
      "----\n",
      "---> Based on the result from the order_status function, I can tell you that your order (Order 8) is currently pending. This means that your large pizza with no toppings has been received by the restaurant and is in the process of being prepared.\n",
      "\n",
      "The status \"pending\" typically means that the order has been accepted but hasn't yet moved to the cooking or delivery stage. The restaurant is likely working on it, and it should progress to the next stages soon.\n",
      "\n",
      "If you'd like to check the status again later or if you have any other questions about your order, please don't hesitate to ask. You can always use your order ID (8) to inquire about the latest status.\n",
      "I want to know the status of order 1\n",
      "{'supervisor': {'next': 'order_status'}}\n",
      "----\n",
      "{'order_status': {'messages': [HumanMessage(content=\"Based on the result from the order_status function, I can inform you that Order 1 is Dispatched.\\n\\nThis means that the order has been prepared and is currently out for delivery. It should be on its way to the delivery address associated with the order.\\n\\nIs there anything else you'd like to know about this order or any other orders?\", name='order_status', id='5d45fa3f-151f-43e8-b477-e7cc643404c7')]}}\n",
      "----\n",
      "---> Based on the result from the order_status function, I can inform you that Order 1 is Dispatched.\n",
      "\n",
      "This means that the order has been prepared and is currently out for delivery. It should be on its way to the delivery address associated with the order.\n",
      "\n",
      "Is there anything else you'd like to know about this order or any other orders?\n"
     ]
    }
   ],
   "source": [
    "interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f9e084",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
